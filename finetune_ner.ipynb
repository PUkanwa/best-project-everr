{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3c287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ukanw\\Documents\\HONOURS-WORK\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from evaluate import load as load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06568d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "MODEL_NAME = \"algiraldohe/lm-ner-linkedin-skills-recognition\"\n",
    "OUTPUT_DIR = \"./ner-finetuned\"\n",
    "MODEL_OUTPUT_DIR = \"./ner-finetuned/model\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 5e-5\n",
    "MAX_LEN = 128\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959cae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_data(path):\n",
    "    examples = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            text = obj[\"text\"]\n",
    "            tokens = text.split()\n",
    "            char_to_token = []\n",
    "            offset = 0\n",
    "            for i, tok in enumerate(tokens):\n",
    "                start_idx = text.find(tok, offset)\n",
    "                # map each character in tok to token index\n",
    "                char_to_token.extend([i] * len(tok))\n",
    "                offset = start_idx + len(tok)\n",
    "                # add mapping for the space (None)\n",
    "                char_to_token.append(None)\n",
    "            labels = [\"O\"] * len(tokens)\n",
    "            for ent in obj.get(\"entities\", []):\n",
    "                s, e = ent[\"start\"], ent[\"end\"] - 1\n",
    "                covered = set(char_to_token[s:e+1])\n",
    "                for idx in covered:\n",
    "                    if idx is None:\n",
    "                        continue\n",
    "                    # if the start of entity corresponds to this token, B- else I-\n",
    "                    if idx == char_to_token[s]:\n",
    "                        labels[idx] = \"B-SKILL\"\n",
    "                    else:\n",
    "                        labels[idx] = \"I-SKILL\"\n",
    "            examples.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d2a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and align labels\n",
    "def tokenize_and_align(examples, tokenizer, label2id):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    aligned_labels = []\n",
    "    for i, tags in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[tags[word_idx]])\n",
    "            else:\n",
    "                lab = tags[word_idx]\n",
    "                # I- for sub-tokens\n",
    "                if lab.startswith(\"B-\"):\n",
    "                    lab = lab.replace(\"B-\", \"I-\")\n",
    "                label_ids.append(label2id[lab])\n",
    "            previous_word_idx = word_idx\n",
    "        aligned_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfafeb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"jobs-new_with_offsets.jsonl\")\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6732a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_list(train)\n",
    "test_ds = Dataset.from_list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69f47c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7c789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"B-SKILL\", \"I-SKILL\"]\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52dcc362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at algiraldohe/lm-ner-linkedin-skills-recognition and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "# use gpu if available\n",
    "# device = 0 if torch.cuda.is_available() else -1\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21381ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47f358f4f1045988ff3ffec49468a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996f9dc329e84298b54cbfcc50fb9179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenized = train_ds.map(\n",
    "    lambda x: tokenize_and_align(x, tokenizer, label2id),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "test_tokenized = test_ds.map(\n",
    "    lambda x: tokenize_and_align(x, tokenizer, label2id),\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8712018",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    # Remove ignored index (-100) and convert to labels\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Handle cases where metric.compute might return None or missing keys\n",
    "    if results is None:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 0.0}\n",
    "\n",
    "    return {\n",
    "        \"precision\": results.get(\"overall_precision\", 0.0),\n",
    "        \"recall\":    results.get(\"overall_recall\", 0.0),\n",
    "        \"f1\":        results.get(\"overall_f1\", 0.0),\n",
    "        \"accuracy\":  results.get(\"overall_accuracy\", 0.0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "554ae37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    seed=SEED,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8134d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c6584b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ukanw\\Documents\\HONOURS-WORK\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 05:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.524100</td>\n",
       "      <td>0.118280</td>\n",
       "      <td>0.863415</td>\n",
       "      <td>0.917098</td>\n",
       "      <td>0.889447</td>\n",
       "      <td>0.972294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.074207</td>\n",
       "      <td>0.924623</td>\n",
       "      <td>0.953368</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.980087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.060844</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.963731</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.981818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.050592</td>\n",
       "      <td>0.984456</td>\n",
       "      <td>0.984456</td>\n",
       "      <td>0.984456</td>\n",
       "      <td>0.988745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.051185</td>\n",
       "      <td>0.979275</td>\n",
       "      <td>0.979275</td>\n",
       "      <td>0.979275</td>\n",
       "      <td>0.987879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.048125</td>\n",
       "      <td>0.979275</td>\n",
       "      <td>0.979275</td>\n",
       "      <td>0.979275</td>\n",
       "      <td>0.987879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.045442</td>\n",
       "      <td>0.979275</td>\n",
       "      <td>0.979275</td>\n",
       "      <td>0.979275</td>\n",
       "      <td>0.987879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ukanw\\Documents\\HONOURS-WORK\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=0.1260528187950452, metrics={'train_runtime': 332.0567, 'train_samples_per_second': 3.614, 'train_steps_per_second': 0.226, 'total_flos': 39196638720000.0, 'train_loss': 0.1260528187950452, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2d26510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ukanw\\Documents\\HONOURS-WORK\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 04:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.045934613794088364,\n",
       " 'eval_precision': 0.9792746113989638,\n",
       " 'eval_recall': 0.9792746113989638,\n",
       " 'eval_f1': 0.9792746113989638,\n",
       " 'eval_accuracy': 0.9878787878787879,\n",
       " 'eval_runtime': 5.734,\n",
       " 'eval_samples_per_second': 17.44,\n",
       " 'eval_steps_per_second': 1.221,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05e86461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./ner-finetuned/model\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "print(\"Model saved to\", MODEL_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "476de103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ukanw\\Documents\\HONOURS-WORK\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics saved to ./ner-finetuned/evaluation_metrics.csv\n",
      "   eval_loss  eval_precision  eval_recall  eval_f1  eval_accuracy  \\\n",
      "0     0.0459          0.9793       0.9793   0.9793         0.9879   \n",
      "\n",
      "   eval_runtime  eval_samples_per_second  eval_steps_per_second  epoch  \n",
      "0         5.779                   17.304                  1.211    3.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Round all float values in eval_results to 4 decimal places\n",
    "eval_results_rounded = {k: round(v, 4) if isinstance(v, float) else v for k, v in eval_results.items()}\n",
    "\n",
    "# Convert the results dictionary to a pandas DataFrame\n",
    "metrics_df = pd.DataFrame([eval_results_rounded])\n",
    "\n",
    "# Define the path for the CSV file\n",
    "metrics_file_path = f\"{OUTPUT_DIR}/evaluation_metrics.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df.to_csv(metrics_file_path, index=False)\n",
    "\n",
    "print(f\"Evaluation metrics saved to {metrics_file_path}\")\n",
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
